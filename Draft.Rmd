---
title: "Predicting Ambient Air Pollution (PM2.5) Across the Contiguous U.S."
author: "Aileen Li, Nyah Strickland"
date: "2023-04-18"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Predicting Ambient Air Pollution (PM2.5) Across the Contiguous U.S.

  Air pollution has consequences for everyone, most especially for those with pre-existing conditions. Air pollution is measured in tiny particles or droplets in the air that are two and one-half microns or less in width called PM2.5. So being able to predict the average PM2.5 concentrations ($\mu$g/m$^3$) will allow us to keep people most vulnerable safe as we look for and implement changes to improve the air quality in the region.
We chose the predictors by …
For our four models, we chose linear regression, k-nearest neighbors, xgboosting, and random forest. Linear regression models a linear relationship between the PM2.5 value in the atmosphere and the predictors. K-nearest neighbors (kNN) returns predicted PM2.5 values based on their neighboring data points. **INSERT BOOSTING MODEL SENTENCE.** Random forest makes an averaged prediction from a collection of independent decision trees. We predict that the RMSE performance of the bootstrapping model will be the best, with a value of around 0.5.  

```{r}
library(tidyverse)
library(tidymodels)
library(broom)
library(xgboost)
library(caret)
library(plotROC)
library(randomForest)
library(factoextra)
library(MASS)
library(ranger)

origin <- read_csv("https://github.com/rdpeng/stat322E_public/raw/main/data/pm25_data.csv.gz")
```

## Wrangling
We split 80% of the origin dataset into training and 20% into testing. Then, we scaled the training datasets for PCA and kNN since both of these require little variance among the variables in the dataset. We omitted the 'id' and 'value' variables from the 'trained_scaled' dataset for PCA because the 'id' values for the monitors would not provide any useful insight on PM2.5 concentration and the 'value' itself is not a predictor. Next, we chose 12 principal components with the highest explained variance since the sum of these components met our threshold of 80% explained variance. Then, we tried to see which variables contributed the most to 'value' and reduced the dimension of the 'trained_scale' dataset based on these variables. For kNN, we created a scaled training set 'train_knn' since extreme values would bias the distance calculation used in kNN modeling. For the other models, the regular 'train' set since the variables' values do not affect their prediction performance. Lastly, we did some exploratory analysis to see the linear relationship between longitude and latitude with PM2.5 concentration.
```{r Predictors}
set.seed(123)
# Split origin dataset into 80/20
dat_split <- initial_split(origin, prop =.8)
dat_split
# Use 80% of data for training 
train <- training(dat_split)

# Create standardize train data for kNN and PCA
# Omit value and id variables since these should not be scaled
# train_scaled for PCA
train_scaled = train[,3:50] %>% dplyr::select(where(is.numeric)) %>% mutate(across(fips:aod, scale)) 
#%>% cbind(state=train$state,county=train$county,city=train$city)

# train_knn for kNN model
train_knn = train[,2:50] %>% dplyr::select(where(is.numeric)) %>% mutate(across(fips:aod, scale)) %>% cbind(state=train$state,county=train$county,city=train$city)

# Calculate principal components on scaled training data
pca_result <- train_scaled %>% dplyr::select(where(is.numeric)) %>%
  prcomp()

# Plot scree plot
pca_result %>% fviz_eig()

# Plot table to show explained variance % by PCs
summary(pca_result)

# Threshold of Explained Variance = 80%
# Calculated the sum of PCs with highest explained variance until threshold was meant
0.3253 + 0.09749 + 0.07984 + 0.06501 + 0.05196 + 0.04147 + 0.03445 +
0.02926 + 0.02563 + 0.0220 + 0.02127 + 0.0188

PC1 <- pca_result$rotation[,1]
PC1_scores <- abs(PC1)
PC1_scores_ordered <- sort(PC1_scores, decreasing = TRUE)
names(PC1_scores_ordered)

# Reduce dimension of dataset
trunc <- pca_result$x[,1:12] %*% t(pca_result$rotation[,1:12])
# Add the center (and re-scale) back to data 
if(all(pca_result$scale != FALSE)){
  trunc <- scale(trunc, center = FALSE , scale=1/pca_result$scale) 
  } 
if(all(pca_result$center != FALSE)){ 
  trunc <- scale(trunc, center = -1 * pca_result$center, scale=FALSE) 
  } 
dim(trunc); dim(train_scaled)

# unhelpful plot -- delete
# plot(pca_result$x[,1], pca_result$x[,2])

# Show which features have the most influence
as_tibble(pca_result$rotation, 
          rownames = "variable") %>% 
    ggplot(aes(variable, PC1)) +
    geom_point() +
    coord_flip()

as_tibble(pca_result$rotation, 
          rownames = "variable") %>% 
    ggplot(aes(variable, PC2)) +
    geom_point() +
    coord_flip()

# Plot clusters between PC1 and PC2 using a biplot
fviz_pca_var(pca_result)

# Reduce dimension of dataset
trunc <- pca_result$x[,1:12] %*% t(pca_result$rotation[,1:12])
# Add the center (and re-scale) back to data 
if(all(pca_result$scale != FALSE)){
  trunc <- scale(trunc, center = FALSE , scale=1/pca_result$scale) 
  } 
if(all(pca_result$center != FALSE)){ 
  trunc <- scale(trunc, center = -1 * pca_result$center, scale=FALSE) 
  } 
dim(trunc); dim(train_scaled)

# Visualize linear relationship between predictors (lat, lon, state) and value
origin %>% ggplot(aes(x=lon, y=value)) + 
  geom_point() + geom_smooth() + 
  labs(title="PM2.5 Concentration vs Longitude", x="Longitude (Degrees)", y="PM2.5 (mu g/m^3)")

origin %>% ggplot(aes(x=lat, y=value)) + 
  geom_point() + geom_smooth() + 
  labs(title="PM2.5 Concentration vs Latitude", x="Latitude (Degrees)", y="PM2.5 (mu g/m^3)")

origin %>% ggplot(aes(x=value, fill=state)) + geom_histogram() + facet_wrap(~state) + 
  labs(title="PM2.5 Concentration", x="PM2.5 (mu g/m^3)", y="Frequency (Monitors)") + theme(legend.position='none')
```

## Predictor/Feature Selection
We used random forest (RF) to determine which variables would be the best predictors for our model since RF is good at using variance to determine which features would be most influential for a model's predictive performance. In other words, the more variability there is in a dataset, the better RF performs at predicting which variables contribute to the outcome by averaging the results of all trees at the end and this averaging reduces the model's variance. Since our dataset has a wide variety of predictors ranging from education attainment levels to poverty levels in a given monitor region to emission data, RF would be good at reducing the variance in our dataset. For a given feature, the lower its impurity levels are, the more important that feature is. As a result, we took the top 10 important variables as input for the training set for our models. 
```{r}
# Use Random Forest for variable selection
rfmodel = randomForest(value~., data=train[,2:50])
import=as.data.frame(randomForest::importance(rfmodel)) |> arrange(desc(IncNodePurity))
import=tibble::rownames_to_column(import, 'variables')

# take top 10 important variables
import10 = import[1:10,]
```

## Models 
```{r LinearRegression}
set.seed(123)
# Fit model and get summary
fit_lm = lm(value ~ CMAQ+lat+county_area+lon+aod+popdens_county+zcta+
             log_nei_2008_pm25_sum_10000+log_nei_2008_pm10_sum_10000, data=train)
step <- stepAIC(fit_lm, direction="backward", trace=TRUE)
# Drops county_area, popdens_county, and log_nei_2008_pm25_sum_10000

# Create the recipe for all models
rec1 <- train %>% 
    recipe(value ~ CMAQ+lat+lon+aod+zcta+log_nei_2008_pm25_sum_10000) 

# Linear regr. model
model1 <- linear_reg() %>% 
    set_engine("lm") %>% 
    set_mode("regression")

wf1 <- workflow() %>% 
    add_recipe(rec1) %>% 
    add_model(model1)
# wf1

res1 <- wf1 |> parsnip::fit(data = train)

# Check performance of linear regression model on the training data
res1 %>% 
    extract_fit_engine() %>% 
    summary()

# Check linear regression model performance using cross-validation
folds1 <- vfold_cv(train, v = 10)
# folds1
res1 <- fit_resamples(wf1, resamples = folds1)
res1 %>% 
    collect_metrics()
```

```{r kNearestNeighbors}
# kNN uses scaled data called train_knn
set.seed(123)
# Find ideal k neighbors
sqrt(nrow(train))

# Create recipe
rec2 = train_knn |> recipe(value ~ CMAQ+lat+county_area+lon+aod+popdens_county+zcta+
             log_nei_2008_pm25_sum_10000+log_nei_2008_pm10_sum_10000)

# Create kNN model
model2 <- nearest_neighbor(neighbors = 26) %>% 
    set_engine("kknn") %>% 
    set_mode("regression")

# Create workflow
  wf2 <- workflow() %>% 
    add_recipe(rec2) %>% 
    add_model(model2)

# Fit the model on the training dataset using the `fit()` function
res2 <- parsnip::fit(wf2, data = train_knn)

# Check performance on the complete training data
res2 %>% 
    extract_fit_engine() %>% 
    summary()

# Check performance using cross-validation
folds2 <- vfold_cv(train, v = 10)
# folds
res2 <- fit_resamples(wf2, resamples = folds2) # ERROR Assigned data `orig_rows` must be compatible with existing data.
res2 %>% 
    collect_metrics()
```

Preprocessing requirements
xgboost does not have a means to translate factor predictors to grouped splits. Factor/categorical predictors need to be converted to numeric values (e.g., dummy or indicator variables) for this engine. When using the formula method via fit.model_spec(), parsnip will convert factor columns to indicators using a one-hot encoding.

```{r Xgboost}
set.seed(123)
# Preprocessing recipe
preprocessing_recipe <- 
  recipes::recipe(value ~ CMAQ+lat+county_area+lon+aod+popdens_county+zcta+
             log_nei_2008_pm25_sum_10000+log_nei_2008_pm10_sum_10000, data = training(dat_split)) %>%
  # Convert categorical variables to factors
  recipes::step_string2factor(all_nominal()) %>%
  # Combine low frequency factor levels
  recipes::step_other(all_nominal(), threshold = 0.01) %>%
  # Remove no variance predictors which provide no predictive information 
  recipes::step_nzv(all_nominal()) %>%
  prep()

folds3 <- recipes::bake(preprocessing_recipe, new_data = training(dat_split)) %>%  
  rsample::vfold_cv(v = 5)

# XGBoost model specification
xgboost_model <- parsnip::boost_tree(mode = "regression", trees = 1000, min_n = tune(),
  tree_depth = tune(), learn_rate = tune(), loss_reduction = tune()) %>%
  set_engine("xgboost", objective = "reg:squarederror")

# Grid specification
xgboost_params <- dials::parameters(min_n(), tree_depth(), learn_rate(), loss_reduction())

xgboost_grid <- dials::grid_max_entropy(xgboost_params, size = 60)

head(xgboost_grid)

xgboost_wf <- workflows::workflow() %>%
  add_model(xgboost_model) %>% 
  add_formula(value ~ CMAQ+lat+county_area+lon+aod+popdens_county+zcta+
             log_nei_2008_pm25_sum_10000+log_nei_2008_pm10_sum_10000)

# Hyperparameter tuning
# Takes a long time to run, 5-8 min
xgboost_tuned <- tune::tune_grid(object = xgboost_wf, resamples = folds3,
  grid = xgboost_grid, metrics = yardstick::metric_set(rmse, rsq),
  control = tune::control_grid(verbose = TRUE))

# Find hyper parameter values which performed best at minimizing RMSE
xgboost_tuned %>% tune::show_best(metric = "rmse") %>%knitr::kable()
# Isolate the best performing hyperparameter values.
xgboost_best_params <- xgboost_tuned %>% tune::select_best("rmse")
xgboost_best_params

# Final boost model
xgboost_model_final <- xgboost_model %>% finalize_model(xgboost_best_params)

# Eval. model performance on train data
train_processed <- bake(preprocessing_recipe,  new_data = training(dat_split))

train_prediction <- xgboost_model_final %>%
  # Fit the model on all the training data
  parsnip::fit(formula = value ~ CMAQ+lat+county_area+lon+aod+popdens_county+zcta+
             log_nei_2008_pm25_sum_10000+log_nei_2008_pm10_sum_10000, data = train_processed) %>%
  # Predict value for the training data
  predict(new_data = train_processed) %>%
  bind_cols(training(dat_split))

xgboost_score_train <- train_prediction %>%
  yardstick::metrics(value, .pred) %>%
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
xgboost_score_train
```

```{r RandomForest}
set.seed(123)
# Preprocessing recipe
rec4 <- recipe(value ~ CMAQ+lat+county_area+lon+aod+popdens_county+zcta+
             log_nei_2008_pm25_sum_10000+log_nei_2008_pm10_sum_10000, data=train) %>%
  step_log(all_outcomes()) |> 
  step_normalize(all_numeric_predictors()) |> 
  step_dummy(all_nominal_predictors())

#  Create model specification and wf
rf_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", num.threads=7, importance="impurity") |> 
  set_mode("regression")

wf4 <- workflow() %>%
  add_recipe(rec4) %>%
  add_model(rf_spec)

folds4 = recipes::bake(preprocessing_recipe, new_data = training(dat_split)) %>%  
  rsample::vfold_cv(v = 5)

# Space-filling designs grid
rf_grid <- grid_latin_hypercube(min_n(), mtry(range = c(4, 9)), trees(), size = 60)

# Tuning hyperparameters
library(doParallel)
all_cores = parallel::detectCores(logical=FALSE)
cl = makePSOCKcluster(all_cores)
doParallel::registerDoParallel(cl)

# Takes long time to load 2 min
tune_res <- wf4 |> tune_grid(resamples = folds4, grid = rf_grid, metrics=metric_set(rmse,rsq), control = tune::control_grid(verbose = TRUE))
tune_res

# Find hyper parameter values which performed best at minimizing RMSE
tune_res %>% tune::show_best(metric = "rmse") %>%knitr::kable()
# Isolate the best performing hyperparameter values.
rf_best_params <- tune_res %>% tune::select_best("rmse")
rf_best_params

# Final Random Forest model
rf_model_final <- rf_spec %>% finalize_model(rf_best_params)

# Check Random Forest model performance
train_processed2 <- bake(preprocessing_recipe,  new_data = training(dat_split))

train_prediction2 = rf_model_final |> 
  parsnip::fit(formula = value ~ CMAQ+lat+county_area+lon+aod+popdens_county+zcta+
             log_nei_2008_pm25_sum_10000+log_nei_2008_pm10_sum_10000, data=train_processed2) |> 
  predict(new_data=train_processed2) |> 
  bind_cols(training(dat_split))
  
rf_score_train = train_prediction2 |> 
  yardstick::metrics(value, .pred) |> 
  mutate(.estimate=format(round(.estimate, 2), big.mark=","))
rf_score_train
```

## Results
Describe the development of your 3 prediction models (or 4 models, if working in a group) and
how you compared their performance. 
Be sure to describe the splitting of training and testing datasets 
and the use of cross-validation to evaluate prediction metrics. 
Remember that the primary metric for your prediction model will be root-mean-squared error (RMSE). 
Your results should include 
at least one visualization demonstrating the prediction performance of your models. 
You should also include 
a table summarizing the prediction metrics (including RMSE) across all of the models that you tried.

1. Based on test set performance, at what locations does your model give predictions that
are closest and furthest from the observed values? What do you hypothesize are the
reasons for the good or bad performance at these locations?
2. What variables might predict where your model performs well or not? For example, are
their regions of the country where the model does better or worse? Are there variables
that are not included in this dataset that you think might improve the model performance
if they were included in your model?
3. There is interest in developing more cost-effect approaches to monitoring air pollution
on the ground. Two candidates for replacing the use of ground-based monitors are
numerical models like CMAQ and satellite-based observations such as AOD. How well do
CMAQ and AOD predict ground-level concentrations of PM2.5? How does the prediction
performance of your model change when CMAQ or aod are included (or not included) in
the model?
4. The dataset here did not include data from Alaska or Hawaii. Do you think your model
will perform well or not in those two states? Explain your reasoning


## Discussion
Putting it all together, what did you learn from your data and your model performance?
• Answer your Question posed above, citing any supporting statistics, visualizations, or results from the data or your models.
• Reflect on the process of conducting this project. What was challenging, what have you learned from the process itself?
• Reflect on the performance of your final prediction model. Did it perform as well as you originally expected? If not, why do you think it didn’t perform as well?
• Include acknowledgments for any help received. If a group project, report the contribution of each member (i.e. who did what?).


