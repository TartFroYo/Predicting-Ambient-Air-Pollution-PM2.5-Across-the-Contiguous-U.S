---
title: "Predicting Ambient Air Pollution (PM2.5) Across the Contiguous U.S."
author: "Aileen Li, Nyah Strickland"
date: "2023-04-18"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Predicting Ambient Air Pollution (PM2.5) Across the Contiguous U.S.

  Air pollution has consequences for everyone, most especially for those with pre-existing conditions. Air pollution is measured in tiny particles or droplets in the air that are two and one-half microns or less in width called PM2.5. So being able to predict the average PM2.5 concentrations ($\mu$g/m$^3$) will allow us to keep people most vulnerable safe as we look for and implement changes to improve the air quality in the region.
We chose the predictors by â€¦
For our four models, we chose linear regression, k-nearest neighbors, xgboosting, and random forest. Linear regression models a linear relationship between the PM2.5 value in the atmosphere and the predictors. K-nearest neighbors (kNN) returns predicted PM2.5 values based on their neighboring data points. INSERT BOOSTING MODEL SENTENCE. Random forest makes an averaged prediction from a collection of independent decision trees. We predict that the RMSE performance of the bootstrapping model will be the best, with a value of around 0.5.

```{r}
library(tidyverse)
library(tidymodels)
library(broom)
library(caret)
library(plotROC)

origin <- read_csv("https://github.com/rdpeng/stat322E_public/raw/main/data/pm25_data.csv.gz")
```

## Wrangling
```{r Predictors}
set.seed(123)
# Split origin dataset into 80/20
dat_split <- initial_split(origin, prop =.8)
dat_split

# Use 80% of data for training 
train <- training(dat_split)

# Create standardize train data for kNN


# Visualize linear relationship between predictors (lat, lon, state) and value
origin |> ggplot(aes(x=lon, y=value)) + 
  geom_point() + geom_smooth() + 
  labs(title="PM2.5 Concentration vs Longitude", x="Longitude (degrees)", y="PM2.5 (mu g/m^3)")

origin %>% ggplot(aes(x=lat, y=value)) + 
  geom_point() + geom_smooth() + 
  labs(title="PM2.5 Concentration vs Latitude", x="Latitude (degrees)", y="PM2.5 (mu g/m^3)")
  
origin %>% ggplot(aes(x=value, fill=state)) + geom_histogram() + facet_wrap(~state) + 
  labs(title="PM2.5 Concentration by State", x="PM2.5 (mu g/m^3)", y="Frequency (Monitors)") +
  theme(legend.position="none",strip.text.x = element_text(margin = margin()))
```

## Predictor Reduction
```{r}
# Scale dataset


# Covar. matrix


# Eigenval.s and eigenvec.s


# PCA

# Data transformation in new dimensional space


```





## Models 
```{r Linear Regression}
set.seed(123)
# Create the recipe for all models
rec <- train %>% 
    recipe(value ~ lat + lon + state) 

# Linear regr. model
model1 <- linear_reg() %>% 
    set_engine("lm") %>% 
    set_mode("regression")

wf1 <- workflow() %>% 
    add_recipe(rec) %>% 
    add_model(model1)
# wf1

res1 <- fit(wf1, data = train)

# Check performance of linear regression model on the training data
res1 %>% 
    extract_fit_engine() %>% 
    summary()

# Check linear regression model performance using cross-validation

folds1 <- vfold_cv(train, v = 10)
folds1
res1 <- fit_resamples(wf1, resamples = folds1)
res1 %>% 
    collect_metrics()
```

```{r kNN}
set.seed(123)
# Find ideal k neighbors
sqrt(nrow(train))

# Scale the training data for k-NN model
train_scaled = train |> dplyr::select(where(is.numeric)) |> mutate(across(fips:aod, scale)) |> cbind(state=train$state,county=train$county,city=train$city)

# Create kNN model
model2 <- nearest_neighbor(neighbors = 26) %>% 
    set_engine("kknn") %>% 
    set_mode("regression")

# Create workflow
  wf2 <- workflow() %>% 
    add_recipe(rec) %>% 
    add_model(model2)

# Fit the model on the training dataset using the `fit()` function
res2 <- fit(wf2, data = train_scaled)

# Check performance on the complete training data
res2 %>% 
    extract_fit_engine() %>% 
    summary()

# Check performance using cross-validation

folds2 <- vfold_cv(train, v = 10)
# folds
res2 <- fit_resamples(wf2, resamples = folds2) # ERROR Assigned data `orig_rows` must be compatible with existing data.
res2 %>% 
    collect_metrics()
```

```{r Boost}
set.seed(123)
# Preprocessing "recipe"
preprocessing_recipe <- 
  recipes::recipe(value~lon+lat, data = training(dat_split)) %>%
  # Convert categorical variables to factors
  recipes::step_string2factor(all_nominal()) %>%
  # Combine low frequency factor levels
  recipes::step_other(all_nominal(), threshold = 0.01) %>%
  # Remove no variance predictors which provide no predictive information 
  recipes::step_nzv(all_nominal()) %>%
  prep()

folds3 <- recipes::bake(preprocessing_recipe, new_data = training(dat_split)) %>%  
  rsample::vfold_cv(v = 5)

# XGBoost model specification
xgboost_model <- parsnip::boost_tree(mode = "regression", trees = 1000, min_n = tune(),
  tree_depth = tune(), learn_rate = tune(), loss_reduction = tune()) %>%
  set_engine("xgboost", objective = "reg:squarederror")

# Grid specification
xgboost_params <- dials::parameters(min_n(), tree_depth(), learn_rate(), loss_reduction())

xgboost_grid <- dials::grid_max_entropy(xgboost_params, size = 60)

knitr::kable(head(xgboost_grid))

xgboost_wf <- workflows::workflow() %>%
  add_model(xgboost_model) %>% 
  add_formula(value~lon+lat)

# Hyperparameter tuning
# Takes a long time to run
xgboost_tuned <- tune::tune_grid(object = xgboost_wf, resamples = folds3,
  grid = xgboost_grid, metrics = yardstick::metric_set(rmse, rsq, mae),
  control = tune::control_grid(verbose = TRUE))

# Find hyper parameter values which performed best at minimizing RMSE
xgboost_tuned %>% tune::show_best(metric = "rmse") %>%knitr::kable()
# Isolate the best performing hyperparameter values.
xgboost_best_params <- xgboost_tuned %>% tune::select_best("rmse")
knitr::kable(xgboost_best_params)

# Final boost model
xgboost_model_final <- xgboost_model %>% finalize_model(xgboost_best_params)

# Eval. model performance on test data
train_processed <- bake(preprocessing_recipe,  new_data = training(dat_split))

train_prediction <- xgboost_model_final %>%
  # Fit the model on all the training data
  fit(formula = value ~ lon+lat, data = train_processed) %>%
  # Predict the sale prices for the training data
  predict(new_data = train_processed) %>%
  bind_cols(training(dat_split))

xgboost_score_train <- train_prediction %>%
  yardstick::metrics(value, .pred) %>%
  mutate(.estimate = format(round(.estimate, 2), big.mark = ","))
knitr::kable(xgboost_score_train)
```

Preprocessing requirements
xgboost does not have a means to translate factor predictors to grouped splits. Factor/categorical predictors need to be converted to numeric values (e.g., dummy or indicator variables) for this engine. When using the formula method via fit.model_spec(), parsnip will convert factor columns to indicators using a one-hot encoding.

```{r Random Forest}
set.seed(123)
# Create Random Forest Model
set.seed(123)
# Use grid of tuning parameters
model4 <- rand_forest(mtry = tune("mtry"),
                     min_n = tune("min_n")) %>% 
    set_engine("ranger") %>% 
    set_mode("regression")
# Create workflow
  wf4 <- workflow() %>% 
    add_recipe(rec) %>% 
    add_model(model4)
  
# Fit the model on the full training dataset using the `fit()` function
res4 <- fit(wf4, data = train_scaled)
# Check performance on the complete training data
res4 %>% 
    extract_fit_engine() %>% 
    summary()
# Check performance using cross-validation
folds4 <- vfold_cv(train, v = 10)
folds4
# Determine performance of model by resampling
res4 <- fit_resamples(wf4, folds4) # ERROR Assigned data `orig_rows` must be compatible with existing data.
res4 %>% 
    collect_metrics()

## Try a grid of tuning parameters
model <- rand_forest(mtry = tune("mtry"),
                     min_n = tune("min_n")) %>% 
    set_engine("ranger") %>% 
    set_mode("regression")

wf4 <- workflow() %>% 
    add_recipe(rec) %>% 
    add_model(model)

## Fit model over grid of tuning parameters
res <- tune_grid(wf4, resamples = folds4, grid = 20)

res %>% 
    collect_metrics()
```

## Results
Describe the development of your 3 prediction models (or 4 models, if working in a group) and
how you compared their performance. 
Be sure to describe the splitting of training and testing datasets 
and the use of cross-validation to evaluate prediction metrics. 
Remember that the primary metric for your prediction model will be root-mean-squared error (RMSE). 
Your results should include 
at least one visualization demonstrating the prediction performance of your models. 
You should also include 
a table summarizing the prediction metrics (including RMSE) across all of the models that you tried.

1. Based on test set performance, at what locations does your model give predictions that
are closest and furthest from the observed values? What do you hypothesize are the
reasons for the good or bad performance at these locations?
2. What variables might predict where your model performs well or not? For example, are
their regions of the country where the model does better or worse? Are there variables
that are not included in this dataset that you think might improve the model performance
if they were included in your model?
3. There is interest in developing more cost-effect approaches to monitoring air pollution
on the ground. Two candidates for replacing the use of ground-based monitors are
numerical models like CMAQ and satellite-based observations such as AOD. How well do
CMAQ and AOD predict ground-level concentrations of PM2.5? How does the prediction
performance of your model change when CMAQ or aod are included (or not included) in
the model?
4. The dataset here did not include data from Alaska or Hawaii. Do you think your model
will perform well or not in those two states? Explain your reasoning


## Discussion
Putting it all together, what did you learn from your data and your model performance?
â€¢ Answer your Question posed above, citing any supporting statistics, visualizations, or results from the data or your models.
â€¢ Reflect on the process of conducting this project. What was challenging, what have you learned from the process itself?
â€¢ Reflect on the performance of your final prediction model. Did it perform as well as you originally expected? If not, why do you think it didnâ€™t perform as well?
â€¢ Include acknowledgments for any help received. If a group project, report the contribution of each member (i.e. who did what?).


